{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "### Anomaly ###\n",
    "# TARGET_FILE = \"../MainExp/Hadoop/data/anomaly/Hadoop_Anomaly_4.2k.log\"\n",
    "# TARGET_FILE = \"../MainExp/hdfs/data/anomaly/HDFS_Anomaly_10k.log\"\n",
    "\n",
    "### Target File List ###\n",
    "root_path = \"../02_loghub-datasets/\"\n",
    "\n",
    "target_file_path_list = [\n",
    "    \"Windows/Windows_2k.log\",\n",
    "    \"Linux/Linux_2k.log\",\n",
    "    \"Hadoop/Hadoop_2k.log\",\n",
    "    \"HDFS/HDFS_2k.log\",\n",
    "    \"Spark/Spark_2k.log\",\n",
    "    \"OpenStack/OpenStack_2k.log\",\n",
    "    \"Apache/Apache_2k.log\",\n",
    "    \"OpenSSH/OpenSSH_2k.log\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_cleaner(line):\n",
    "    # print(line)\n",
    "\n",
    "    concat_char = \"\"\n",
    "\n",
    "    for char in line:\n",
    "        is_num = False\n",
    "        is_big = False\n",
    "        is_small = False\n",
    "\n",
    "        try:\n",
    "            # print (\"now char\",char)\n",
    "            if char.isdigit():\n",
    "                concat_char+=char\n",
    "            elif char.islower():\n",
    "                concat_char+=char\n",
    "            elif char.isupper():\n",
    "                char=char.lower()\n",
    "                concat_char+=char\n",
    "            else:\n",
    "                if concat_char[-1] == \" \":\n",
    "                    pass\n",
    "                else:\n",
    "                    concat_char+=\" \"\n",
    "                pass\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return concat_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_clean_dataset(target_file_path, dst_dir_path, file_cnt, remove_timestamp=0):\n",
    "    if not os.path.exists(dst_dir_path):\n",
    "        os.makedirs(dst_dir_path)\n",
    "        \n",
    "    cnt = 0\n",
    "    while True:\n",
    "        cnt += 1\n",
    "        src = target_file_path\n",
    "        dst = os.path.join(dst_dir_path, f\"./concordance_{cnt}_clean.txt\")\n",
    "\n",
    "        file =  open(src, \"r\")\n",
    "        lines = file.readlines()\n",
    "        result_file = open(dst, \"w\")\n",
    "\n",
    "        for line in lines:\n",
    "            line= line.strip()\n",
    "            word_list = line.split()\n",
    "            word_list = word_list[remove_timestamp:]\n",
    "            result_file.write(\" \".join(word_list) + \"\\n\")\n",
    "        \n",
    "        result_file.close()\n",
    "\n",
    "        if cnt == file_cnt:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_file_path in target_file_path_list:\n",
    "    print(f\"Processing file: {target_file_path}\")\n",
    "    file = open(os.path.join(root_path, target_file_path), \"r\")\n",
    "    lines = file.readlines()\n",
    "\n",
    "    result_file_name = os.path.split(target_file_path)[-1]\n",
    "    result_file_path = \"cleaned_datasets/\"\n",
    "    result_file = open(os.path.join(result_file_path, result_file_name + \"_clean.txt\"), \"w\")\n",
    "\n",
    "    target_line = \"\"\n",
    "    for line in lines:\n",
    "        target_line = line.strip()\n",
    "        cleaned_line = line_cleaner(line.strip())\n",
    "        # print(f\"Original Line: {line.strip()}\")\n",
    "        # print(f\"Cleaned Line: {cleaned_line}\")\n",
    "        result_file.write(cleaned_line + \"\\n\")\n",
    "\n",
    "    result_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_clean_dataset(target_file_path=\"cleaned_datasets/Linux_2k.log_clean.txt\",dst_dir_path=\"../03_exp-linux/data/clean/\",file_cnt=20, remove_timestamp=5)\n",
    "make_clean_dataset(target_file_path=\"cleaned_datasets/Windows_2k.log_clean.txt\",dst_dir_path=\"../04_exp-windows/data/clean/\",file_cnt=20, remove_timestamp=6)\n",
    "make_clean_dataset(target_file_path=\"cleaned_datasets/Hadoop_2k.log_clean.txt\",dst_dir_path=\"../05_exp-hadoop/data/clean/\",file_cnt=20, remove_timestamp=7)\n",
    "make_clean_dataset(target_file_path=\"cleaned_datasets/OpenStack_2k.log_clean.txt\",dst_dir_path=\"../07_exp-openstack/data/clean/\",file_cnt=20, remove_timestamp=18)\n",
    "make_clean_dataset(target_file_path=\"cleaned_datasets/Apache_2k.log_clean.txt\",dst_dir_path=\"../09_exp-apache/data/clean/\",file_cnt=20, remove_timestamp=7)\n",
    "make_clean_dataset(target_file_path=\"cleaned_datasets/OpenSSH_2k.log_clean.txt\",dst_dir_path=\"../10_exp-openssh/data/clean/\",file_cnt=20, remove_timestamp=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "log-nlp_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "66d4fb87ddb4494270580ee563fb6840ad68ac8d4139104e541e8f4c217ec4ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
